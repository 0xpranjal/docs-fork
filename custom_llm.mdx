---
title: "Custom LLMs"
sidebarTitle: "Custom LLMs"
description: "Learn how to set up your server to act as the LLM."
---

If you want more control over the LLM, you can use the `custom` model provider. Instead of hitting OpenAI's API, the assistant will make requests to your server, and your server will be responsible for making requests to an LLM.

You might want to set this up to:

- Use your own LLM, like LLAMA
- Update the context during the conversation
- Customize the messages before they're sent to an LLM

To set up a use a custom LLM, you can specify the URL in the assistant body:

```json
{
  "name": "My Assistant",
  "model": {
    "provider": "custom",
    "url": "https://my-server.com/llm"
  }
}
```

After each of the user's statements, the assistant will make a `POST` request to your server with the following body:

```json
{
  "call": { Call Object },
  "messages": [
    {
      "role": "bot",
      "message": "How can I help?",
      "time": 1700747423707
    },
    {
      "role": "user",
      "message": "What's the weather?"
      "time": 1700747425787
    },
    ...
  ]
}
```

The `messages` will only include the spoken transcript. Any messages that have been interrupted will not be included.

Your server should respond with a JSON object containing what the assistant should say next. For example:

```json
{ "message": "It's sunny today." }
```
